[["from-json-to-csv.html", "Social Network Analysis Chapter 1 From JSON to CSV 1.1 Configuration 1.2 Libraries 1.3 Defining functions 1.4 From JSON to CSV", " Social Network Analysis Pedro Antonio Benito Rojano 2022-06-10 Chapter 1 From JSON to CSV We have a dataset in JSONs files, but the suitable type of file is CSV. So we need to convert the JSONs to a CSV file. Steps: Load the JSONs files. Convert JSONs to dataset. Save dasaset as CSV file. 1.1 Configuration Set the path to import the JSONs files. path_import_jsons &lt;- &quot;data/json&quot; Set the path to export the dataset as CSV. path_export_csv &lt;- &quot;data/original.csv&quot; 1.2 Libraries Library for pipe working. library(dplyr) Library for manipulate JSONs. library(tidyjson) Libraries for parallel execution. library(parallel) 1.3 Defining functions 1.3.1 Auxiliary functions Define function for list a forlder. list_folder &lt;- function(path){ list.files(path = path) %&gt;% sapply(function(json) paste(path, json, sep=&quot;/&quot;)) } Define function for transform JSON to a dataset json_to_df &lt;- function(path){ path %&gt;% read_json() %&gt;% spread_all() %&gt;% tibble() } Define function for generate a dataset from a folder with JSONs files. jsons_folder_to_df &lt;- function(path){ path %&gt;% list_folder() %&gt;% lapply(json_to_df) %&gt;% bind_rows() } Define function for save a data frame to CSV. save_df_to_csv &lt;- function(df, path){ df %&gt;% lapply(as.character) %&gt;% write.csv(path_export_csv) } Define function for generate a CSV datset from a JSON folder. from_json_to_csv &lt;- function(json_folder_path, csv_path){ jsons_folder_to_df(json_folder_path) %&gt;% save_df_to_csv(csv_path) } 1.3.2 Parallel Define function for setup the cluster, with needed libraries setup_cluster &lt;- function() { cluster &lt;- detectCores() %&gt;% makeCluster() clusterEvalQ(cluster, expr = library(tidyjson)) clusterEvalQ(cluster, expr = library(dplyr)) return(cluster) } Define function for generate a dataset from a folder with JSONs files by parallel execution. jsons_folder_to_df_parallel &lt;- function(cluster, path){ json_paths &lt;- list_folder(path) df &lt;- cluster %&gt;% parLapply(json_paths, json_to_df) %&gt;% bind_rows() json_paths return(df) } Define function for generate a CSV datset from a JSON folder by parallel execution. from_json_to_csv_parallel &lt;- function(json_folder_path, csv_path){ cluster &lt;- setup_cluster() cluster %&gt;% jsons_folder_to_df_parallel(json_folder_path) %&gt;% save_df_to_csv(csv_path) stopCluster(cluster) } 1.4 From JSON to CSV Using defined functions for converting from JSON to CSV. from_json_to_csv_parallel(path_import_jsons, path_export_csv) "],["preprocessing-dataset.html", "Chapter 2 Preprocessing Dataset 2.1 Paths 2.2 Libraries 2.3 Load dataset 2.4 Statistics 2.5 Inspecting dataframe 2.6 Replacing values 2.7 New &amp; modifiead colums 2.8 Save dataframe 2.9 Functions for preprocessing", " Chapter 2 Preprocessing Dataset 2.1 Paths Set the path of the dataframe file. path_import = &quot;data/original.csv&quot; path_export = &quot;data/preprocessed.csv&quot; 2.2 Libraries Library for read dataset. library(readr) Library for data frames processing. library(dplyr) library(tidyr) Library for R Markdown. library(rmarkdown) library(knitr) Library for data presentation. library(scales) Library for manage strings. library(stringr) 2.3 Load dataset Load dataset. df &lt;- read_csv(path_import) 2.4 Statistics Dimensions. dim(df) ## [1] 183 447 2.4.1 Types View witch types are in the dataset. col_types_all &lt;- df %&gt;% sapply(typeof) %&gt;% unlist() col_types_table &lt;- col_types_all %&gt;% table() col_types &lt;- col_types_table %&gt;% as.vector() names(col_types) &lt;- names(col_types_table) ## character double logical ## 158 204 85 As can be seen there are the three expected types: character, double and logical. 2.4.2 NA 2.4.2.1 Percentaje of NA values Define function to see the amount of NA values in the dataframe. percent_of_NA &lt;- function(df){ num_of_NA &lt;- df %&gt;% is.na() %&gt;% sum() num_of_values &lt;- df %&gt;% dim() %&gt;% prod() percent_of_NA &lt;- (num_of_NA / num_of_values) %&gt;% percent() return(percent_of_NA) } percent_of_NA(df) ## [1] &quot;37%&quot; 2.4.2.2 Columns with NA Define functions to see the NA in columns. num_of_NA_by_column &lt;- function(df){ df %&gt;% is.na() %&gt;% colSums() } remove_0 &lt;- function(x) x[x!=0] names_of_colums_with_NA &lt;- function(df) df %&gt;% num_of_NA_by_column() %&gt;% remove_0 %&gt;% names() percentaje_of_cols_with_NA &lt;- function(df) (length(names_of_colums_with_NA(df)) / ncol(df)) %&gt;% percent() Compute the percentaje of cols with NA. percentaje_of_cols_with_NA(df) ## [1] &quot;74%&quot; Inspect if there are columns full of NA. is_full_of_NA &lt;- function(col){ num_of_NA &lt;- col %&gt;% is.na() %&gt;% sum() return(num_of_NA == length(col)) } cols_full_of_NA &lt;- df %&gt;% select_if(is_full_of_NA) %&gt;% names() ## [1] &quot;authentihash&quot; &quot;scans.Bkav.result&quot; &quot;scans.CMC.result&quot; ## [4] &quot;scans.ALYac.result&quot; &quot;scans.Malwarebytes.result&quot; &quot;scans.K7AntiVirus.result&quot; ## [7] &quot;scans.Baidu.result&quot; &quot;scans.SUPERAntiSpyware.result&quot; &quot;scans.Gridinsoft.result&quot; ## [10] &quot;scans.ViRobot.result&quot; &quot;scans.BitDefenderTheta.result&quot; &quot;scans.TACHYON.result&quot; ## [13] &quot;scans.VBA32.result&quot; &quot;scans.Zoner.result&quot; &quot;scans.Panda.result&quot; ## [16] &quot;scans.Elastic.result&quot; &quot;scans.Cylance.result&quot; &quot;scans.SentinelOne.result&quot; As can be seen there are many columns that are full of NA, so can be deleted. df &lt;- select(df, -all_of(cols_full_of_NA)) 2.4.2.3 Colums with the same value Maybe there are columns that has the same value along all the vector, so are useless. Define function to remove these columns. different_values &lt;- function(x) x %&gt;% na.omit() %&gt;% unique() %&gt;% length() remove_columns_with_the_same_value &lt;- function(df) select_if(df, function(col) different_values(col) &gt; 1) Apply function. num_of_cols_after_remove &lt;- df %&gt;% remove_columns_with_the_same_value() %&gt;% ncol() Calculate the number of columns with same value. ncol(df) - num_of_cols_after_remove ## [1] 147 Awesom! Many colums found. Let’s remove them. df &lt;- remove_columns_with_the_same_value(df) 2.5 Inspecting dataframe Now let’s deeply inspect into the dataframe. 2.5.1 View dataframe View dataframe. 2.5.2 Renaming The column “…1” is the row number, so “n” will be a better name. The “…JSON” it’s a bad name, just “json” is fine. df &lt;- df %&gt;% rename(n = ...1, json=..JSON) 2.5.3 Removing cols There are many duplicated cols, hashes &amp; dates that can be removed, also many useless. 2.5.3.1 Dates There are many dates in the dataset, that are not relevant for virus analysis. So let’s remove them. Define a function for check if a col is of type Date. not &lt;- function(x) !x get_element &lt;- function(x, index) x[index] is_date_col &lt;- function(col, pattern=&quot;^[:digit:]{4}[-:/][:digit:]{2}[-:/][:digit:]{2}&quot;) col %&gt;% as.character() %&gt;% na.omit() %&gt;% get_element(1) %&gt;% str_detect(pattern) Columns detected. df %&gt;% select_if(is_date_col) %&gt;% head() %&gt;% paged_table() Define function for remove cols by a predicate. remove_col_if &lt;- function(df, fun){ cols_to_delete &lt;- df %&gt;% select_if(fun) %&gt;% colnames() df &lt;- df %&gt;% select(-cols_to_delete) return(df) } Remove them. df &lt;- remove_col_if(df, is_date_col) 2.5.3.2 Hashes There are many hashes cols that don’t really provide useful information. So remove them. hashes &lt;- c(&quot;&quot;) df &lt;- df %&gt;% select( -vhash, -sha256, -sha1, -scan_id, -ssdeep, -md5, -additional_info.androguard.certificate.serialnumber, -additional_info.androguard.certificate.thumbprint, -additional_info.exiftool.ZipCRC ) 2.5.3.3 Scans There are many scans of different antivirus, that has very similar information. Just keep the scan with less NA values. Get the best col. scans_col_names &lt;- df %&gt;% colnames() %&gt;% str_match_all(&quot;scans.[:alpha:]*.result&quot;) %&gt;% unlist() scan_na_by_col &lt;- df %&gt;% select(all_of(scans_col_names)) %&gt;% num_of_NA_by_column() scan_col_witch_min_na &lt;- scan_na_by_col %&gt;% which.min() best_scanner_colname &lt;- scan_na_by_col %&gt;% names() %&gt;% get_element(scan_col_witch_min_na) best_scanner_col &lt;- df %&gt;% select(all_of(best_scanner_colname)) best_scanner_name &lt;- best_scanner_colname %&gt;% str_split(&quot;[.]&quot;) %&gt;% unlist() %&gt;% get_element(2) ## [1] &quot;Best scan col: scans.Fortinet.result&quot; ## [1] &quot;Best scanner: Fortinet&quot; Drop all scans but the best. col_index_scanners &lt;- df %&gt;% colnames() %&gt;% str_detect(&quot;scans&quot;) %&gt;% unlist() %&gt;% which() df &lt;- df %&gt;% select(-all_of(col_index_scanners)) %&gt;% cbind(best_scanner_col) 2.5.3.4 Individual columns Reasons: json column contains all the row as JSON. permalink is the URL where Virus Total has the virus file. Main.Activity &amp; Package are strings with all different values. FileTypeExtension, ZipFileName &amp; MIMEType has same values as FileType. ZipBitFlag doesn’t seems to matter. additional_info.magic has the vesion of the ZIP file, that doesn’t seems to matter. Subject.DN is the JSON fragment that has all the information about the subject, but these data are decomposed in the rest of Subject cols. df &lt;- df %&gt;% select( -json, -permalink, -additional_info.androguard.AndroidApplicationInfo, -additional_info.androguard.Main.Activity, -additional_info.exiftool.MIMEType, -additional_info.exiftool.FileTypeExtension, -additional_info.exiftool.ZipFileName, -additional_info.magic, -additional_info.androguard.Package, -additional_info.androguard.certificate.Subject.DN, -additional_info.compressedview.uncompressed_size ) 2.5.3.5 Groups of columns Define a function to remove cols which name match a pattern. remove_cols_which_name_match &lt;- function(df, pattern){ cols_to_remove &lt;- df %&gt;% colnames() %&gt;% str_which(pattern) df_removed_cols &lt;- df %&gt;% select(-all_of(cols_to_remove)) return(df_removed_cols) } Remove groups. Reasons: Issuer group has the same information as Subject group. CompressedView and RiskIndicator.APK groups have the same information as file_type group df &lt;- df %&gt;% remove_cols_which_name_match(&quot;^additional_info.androguard.certificate.Issuer.[:alpha:]*$&quot;) %&gt;% remove_cols_which_name_match(&quot;^additional_info.compressedview.extensions.[:alpha:]*$&quot;) %&gt;% remove_cols_which_name_match(&quot;^additional_info.androguard.RiskIndicator.APK.[:alpha:]*$&quot;) 2.5.4 View results Define a function for sort columns. sort_cols &lt;- function(df){ df &lt;- df %&gt;% select(order(colnames(df))) additionalInfo_cols_logical &lt;- df %&gt;% colnames() %&gt;% str_detect(&quot;additional_info&quot;) additionalInfo_cols &lt;- df %&gt;% select(which(additionalInfo_cols_logical)) not_additionalInfo_cols &lt;- df %&gt;% select(which(!additionalInfo_cols_logical)) %&gt;% select(n, size, everything()) return(cbind(not_additionalInfo_cols, additionalInfo_cols)) } Sort columns. df &lt;- sort_cols(df) View results. 2.6 Replacing values 2.6.1 Replace “Unknown” and “?” by NA There are some columns that hast the value “Unknown” or “?” instead of NAs. So let’s replace them. Define a function to replace values in cols that satisfy a predicate. replace_when &lt;- function(df, fun, value, replacement){ cols_to_replace &lt;- df %&gt;% select_if(fun) %&gt;% colnames() df_replaced_cols &lt;- df %&gt;% select(all_of(cols_to_replace)) %&gt;% sapply(function(col) replace(col, which(col==value), replacement)) df_without_replaced_cols &lt;- df %&gt;% select(-all_of(cols_to_replace)) return(cbind(df_without_replaced_cols, df_replaced_cols)) } Replace ? and Unknown for NAs. df &lt;- df %&gt;% replace_when(function(col) any(str_detect(col, fixed(&quot;Unknown&quot;))), &quot;Unknown&quot;, NA) %&gt;% replace_when(function(col) any(str_detect(col, fixed(&quot;?&quot;))), &quot;?&quot;, NA) 2.6.2 Replace NA for 0 2.6.2.1 Define functions Define functions for replace NAs. replace_na_which_colname_match &lt;- function(df, pattern, replacement){ cols_to_replace &lt;- df %&gt;% colnames() %&gt;% str_which(pattern) df_replaced_cols &lt;- df %&gt;% select(all_of(cols_to_replace)) %&gt;% sapply(function(col) replace_na(col, replacement)) df_without_replaced_cols &lt;- df %&gt;% select(-all_of(cols_to_replace)) return(cbind(df_without_replaced_cols, df_replaced_cols)) } replace_na_when &lt;- function(df, fun, replacement){ cols_to_replace &lt;- df %&gt;% select_if(fun) %&gt;% colnames() df_replaced_cols &lt;- df %&gt;% select(all_of(cols_to_replace)) %&gt;% sapply(function(col) replace_na(col, replacement)) df_without_replaced_cols &lt;- df %&gt;% select(-all_of(cols_to_replace)) return(cbind(df_without_replaced_cols, df_replaced_cols)) } 2.6.2.2 Indiviual columns The columns AndroidApplication and APK.SHARED.LIBRARIES shoud have 0s insteas of NAs. df &lt;- df %&gt;% replace_na_which_colname_match(&quot;AndroidApplication&quot;, 0) %&gt;% replace_na_which_colname_match(&quot;APK.SHARED.LIBRARIES&quot;, 0) 2.6.2.3 Groups of colums The permissions (PERM) and the file types (file_types) groups of columns, seems that there are NAs where there should be 0s. So it would be better to replace them. df &lt;- df %&gt;% replace_na_which_colname_match(&quot;PERM&quot;, 0) %&gt;% replace_na_which_colname_match(&quot;file_types&quot;, 0) 2.6.3 View results Sort columns. df &lt;- sort_cols(df) View results. 2.7 New &amp; modifiead colums There are columns that must be only one, others provide more information by operating two columns, or summarises information. 2.7.1 New total permissions column Create a new column that sums all permissions of permissions columns. pattern &lt;- &quot;additional_info.androguard.RiskIndicator.PERM&quot; df_without_permissions &lt;- df %&gt;% select(., -(str_which(colnames(.), pattern))) df_permissions &lt;- df %&gt;% select(., str_which(colnames(.), pattern)) %&gt;% mutate(., total_PERMs = rowSums(.)) df &lt;- cbind(df_without_permissions, df_permissions) 2.7.2 Merge both MP3 columns into one There are two mp3 extension columns, one for .MP3 and the other for .mp3, both are mp3 files. The others columns has the name of the extension in upper case, so let’s sum both into the MP3 column. mp3_cols_logical &lt;- df %&gt;% colnames() %&gt;% tolower() %&gt;% str_detect(&quot;mp3&quot;) mp3_upper_colname &lt;- colnames(df)[which(mp3_cols_logical)] %&gt;% str_match(&quot;^.*MP3$&quot;) %&gt;% unlist() %&gt;% na.omit() sprintf(&quot;MP3 colname: %s&quot;, mp3_upper_colname) ## [1] &quot;MP3 colname: additional_info.compressedview.file_types.MP3&quot; Sum them into additional_info.compressedview.file_types.MP3 mp3_col &lt;- df %&gt;% select(which(mp3_cols_logical)) %&gt;% rowSums(na.rm = TRUE) df &lt;- df %&gt;% select(-which(mp3_cols_logical)) %&gt;% mutate(additional_info.compressedview.file_types.MP3 = mp3_col) 2.7.3 Express increased uncompressed size as percentage There are two columns that refers to ZIP size, one for the compressed size and the other for the uncompressed size. It would be easier to compare them with the percentage of size increased after decompressed it. increased_size_after_unzip &lt;- df$additional_info.exiftool.ZipUncompressedSize / df$additional_info.exiftool.ZipCompressedSize df &lt;- df %&gt;% select(-additional_info.exiftool.ZipUncompressedSize, -additional_info.exiftool.ZipCompressedSize) %&gt;% mutate(additional_info.exiftool.ZipIncreasedUncompressedSize = increased_size_after_unzip) 2.7.4 Suspicious heuristic as logical The column additional_info.trendmicro.housecall.heuristic has a suspicious flag for some rows, the others are NA. So will be better to make this column logical: NA -&gt; FALSE Suspicious -&gt; True Also rename it to make it more understandable: additional_info.trendmicro.housecall.heuristic -&gt; additional_info.suspicious suspicious &lt;- df$additional_info.trendmicro.housecall.heuristic %&gt;% is.na() %&gt;% not() df &lt;- df %&gt;% select(-additional_info.trendmicro.housecall.heuristic) %&gt;% mutate(additional_info.suspicious = suspicious) 2.7.5 Decompose unpacker he column additional_info.f.prot.unpacker has two possible values: “appended” and “UTF-8”, and when both happen they are combined into “appended, UTF-8”. So will be better to split this column into one logical for each of them. Compute columns. unpacker_appended &lt;- df$additional_info.f.prot.unpacker %&gt;% sapply(function(str) str_detect(str, &quot;appended&quot;)) %&gt;% replace_na(FALSE) unpacker_utf8 &lt;- df$additional_info.f.prot.unpacker %&gt;% sapply(function(str) str_detect(str, &quot;UTF-8&quot;)) %&gt;% replace_na(FALSE) Add them to dataframe. df &lt;- df %&gt;% select(-additional_info.f.prot.unpacker) %&gt;% mutate(additional_info.f.prot.unpacker.appended = unpacker_appended, additional_info.f.prot.unpacker.UTF8 = unpacker_utf8) 2.7.6 Decompose trid The column additional_info.trid has percentages of archives. df$additional_info.trid[1] ## [1] &quot;Android Package (57.0%)\\r\\nJava Archive (20.0%)\\r\\nSweet Home 3D design (generic) (15.5%)\\r\\nZIP compressed archive (5.9%)\\r\\nPrintFox/Pagefox bitmap (640x800) (1.4%)&quot; Will be better to decompose this column into a column with the percentage for each value. 2.7.6.1 Extract values and percentages trid_split_lines &lt;- df$additional_info.trid %&gt;% sapply(function(str) str_split(str, &quot;\\n&quot;)) trid_split_lines[[3]] ## [1] &quot;Java Archive (72.9%)\\r&quot; &quot;ZIP compressed archive (21.6%)\\r&quot; ## [3] &quot;PrintFox/Pagefox bitmap (640x800) (5.4%)&quot; trid_split_value_percentage &lt;- trid_split_lines %&gt;% sapply(function(row) sapply(row, function(str) str_split(str, &quot; \\\\(|%\\\\)&quot;))) trid_split_value_percentage[[1]][[3]] ## [1] &quot;Sweet Home 3D design&quot; &quot;generic)&quot; &quot;15.5&quot; &quot;\\r&quot; trid_values &lt;- trid_split_value_percentage %&gt;% sapply(function(row) sapply(row, function(value_percentage) get_element(value_percentage, 1))) trid_values[[1]] ## Android Package (57.0%)\\r Java Archive (20.0%)\\r ## &quot;Android Package&quot; &quot;Java Archive&quot; ## Sweet Home 3D design (generic) (15.5%)\\r ZIP compressed archive (5.9%)\\r ## &quot;Sweet Home 3D design&quot; &quot;ZIP compressed archive&quot; ## PrintFox/Pagefox bitmap (640x800) (1.4%) ## &quot;PrintFox/Pagefox bitmap&quot; trid_percentage &lt;- trid_split_value_percentage %&gt;% sapply(function(row) sapply(row, function(value_percentage) as.double(get_element(value_percentage, length(value_percentage) - 1)))) trid_percentage[[1]] ## Android Package (57.0%)\\r Java Archive (20.0%)\\r ## 57.0 20.0 ## Sweet Home 3D design (generic) (15.5%)\\r ZIP compressed archive (5.9%)\\r ## 15.5 5.9 ## PrintFox/Pagefox bitmap (640x800) (1.4%) ## 1.4 Define a function to convert the name to a proper trid colname. trid_to_colname &lt;- function(name) name %&gt;% str_to_title() %&gt;% str_match_all(&quot;[:alpha:]*&quot;) %&gt;% unlist() %&gt;% paste0(collapse = &quot; &quot;) %&gt;% str_remove_all(&quot; &quot;) %&gt;% sapply(function(name) paste(&quot;additional_info.trid&quot;, name, sep = &quot;.&quot;)) Define function for rename the percentages. rename_as_value &lt;- function(v){ values_names &lt;- names(v) %&gt;% sapply(function(name) str_split(name, &quot; \\\\(&quot;)) %&gt;% sapply(function(name_split) get_element(name_split, 1)) %&gt;% sapply(trid_to_colname) names(v) &lt;- values_names return(v) } trid_percentage[[1]] %&gt;% rename_as_value() ## additional_info.trid.AndroidPackage additional_info.trid.JavaArchive ## 57.0 20.0 ## additional_info.trid.SweetHomedDesign additional_info.trid.ZipCompressedArchive ## 15.5 5.9 ## additional_info.trid.PrintfoxPagefoxBitmap ## 1.4 Rename the percentages. trid_percentage_names_as_value &lt;- trid_percentage %&gt;% sapply(rename_as_value) trid_percentage_names_as_value[[1]] ## additional_info.trid.AndroidPackage additional_info.trid.JavaArchive ## 57.0 20.0 ## additional_info.trid.SweetHomedDesign additional_info.trid.ZipCompressedArchive ## 15.5 5.9 ## additional_info.trid.PrintfoxPagefoxBitmap ## 1.4 2.7.6.2 Create columns Get columns names. trid_labels &lt;- trid_values %&gt;% unlist() %&gt;% unique() trid_labels ## [1] &quot;Android Package&quot; &quot;Java Archive&quot; ## [3] &quot;Sweet Home 3D design&quot; &quot;ZIP compressed archive&quot; ## [5] &quot;PrintFox/Pagefox bitmap&quot; &quot;Opera Widget&quot; ## [7] &quot;Mozilla Archive Format&quot; &quot;Dalvik Dex class&quot; ## [9] &quot;OpenOffice Extension&quot; &quot;VYM Mind Map&quot; ## [11] &quot;Mozilla Firefox browser extension&quot; &quot;Konfabulator widget&quot; trid_values_colnames &lt;- trid_labels %&gt;% sapply(trid_to_colname) trid_values_colnames ## Android Package.AndroidPackage ## &quot;additional_info.trid.AndroidPackage&quot; ## Java Archive.JavaArchive ## &quot;additional_info.trid.JavaArchive&quot; ## Sweet Home 3D design.SweetHomedDesign ## &quot;additional_info.trid.SweetHomedDesign&quot; ## ZIP compressed archive.ZipCompressedArchive ## &quot;additional_info.trid.ZipCompressedArchive&quot; ## PrintFox/Pagefox bitmap.PrintfoxPagefoxBitmap ## &quot;additional_info.trid.PrintfoxPagefoxBitmap&quot; ## Opera Widget.OperaWidget ## &quot;additional_info.trid.OperaWidget&quot; ## Mozilla Archive Format.MozillaArchiveFormat ## &quot;additional_info.trid.MozillaArchiveFormat&quot; ## Dalvik Dex class.DalvikDexClass ## &quot;additional_info.trid.DalvikDexClass&quot; ## OpenOffice Extension.OpenofficeExtension ## &quot;additional_info.trid.OpenofficeExtension&quot; ## VYM Mind Map.VymMindMap ## &quot;additional_info.trid.VymMindMap&quot; ## Mozilla Firefox browser extension.MozillaFirefoxBrowserExtension ## &quot;additional_info.trid.MozillaFirefoxBrowserExtension&quot; ## Konfabulator widget.KonfabulatorWidget ## &quot;additional_info.trid.KonfabulatorWidget&quot; Create an empty tibble with colnames set. df_trid &lt;- matrix(nrow = nrow(df), ncol = length(trid_values_colnames)) %&gt;% as_tibble(.name_repair = ~ trid_values_colnames) %&gt;% mutate_each(as.double) Colnames. df_trid %&gt;% colnames() ## [1] &quot;additional_info.trid.AndroidPackage&quot; ## [2] &quot;additional_info.trid.JavaArchive&quot; ## [3] &quot;additional_info.trid.SweetHomedDesign&quot; ## [4] &quot;additional_info.trid.ZipCompressedArchive&quot; ## [5] &quot;additional_info.trid.PrintfoxPagefoxBitmap&quot; ## [6] &quot;additional_info.trid.OperaWidget&quot; ## [7] &quot;additional_info.trid.MozillaArchiveFormat&quot; ## [8] &quot;additional_info.trid.DalvikDexClass&quot; ## [9] &quot;additional_info.trid.OpenofficeExtension&quot; ## [10] &quot;additional_info.trid.VymMindMap&quot; ## [11] &quot;additional_info.trid.MozillaFirefoxBrowserExtension&quot; ## [12] &quot;additional_info.trid.KonfabulatorWidget&quot; Types. df_trid %&gt;% sapply(is.double) %&gt;% sum() == ncol(df_trid) ## [1] TRUE Insert values. for(row_index in 1:nrow(df)){ row &lt;- trid_percentage_names_as_value[[row_index]] for(percentage_index in 1:length(row)){ percentage &lt;- row[percentage_index] colname &lt;- names(percentage) df_trid[row_index, colname] &lt;- percentage } } Replace NA for 0. df_trid &lt;- df_trid %&gt;% replace(is.na(.), 0) 2.7.6.3 Merge dataframes Finally merge df and df_trid into one. df &lt;- df %&gt;% select(-additional_info.trid) %&gt;% cbind(df_trid) 2.7.7 View results Sort columns. df &lt;- sort_cols(df) View results. 2.8 Save dataframe After all preprocessing let’s save it into CSV. write.csv(df, path_export) 2.9 Functions for preprocessing 2.9.1 As factor labels &lt;- function(n){ if(n == 5){ return(c(&quot;very low&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;high&quot;, &quot;very high&quot;)) }else if(n == 4){ return(c(&quot;very low&quot;, &quot;low&quot;, &quot;high&quot;, &quot;very high&quot;)) }else if(n == 3){ return(c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;)) }else if(n == 2){ return(c(&quot;low&quot;, &quot;high&quot;)) }else{ stop(&quot;Not avalible&quot;) } } cut_by_quantiles &lt;- function(col){ quantiles &lt;- col %&gt;% quantile(na.rm = TRUE) %&gt;% unique() if(length(quantiles) &gt; 2){ col &lt;- col %&gt;% cut(breaks = quantiles, labels = labels(length(quantiles)-1), include.lowest = TRUE) } return(col) } df_cut_by_quantiles &lt;- function(df){ df_without_numeric &lt;- df[sapply(df, function(col) !is.numeric(col))] df_numeric &lt;- df %&gt;% select_if(is.numeric) df_numeric &lt;- df_numeric %&gt;% lapply(cut_by_quantiles) return(cbind(df_without_numeric, df_numeric)) } "],["clustering.html", "Chapter 3 Clustering 3.1 Libraries 3.2 Configuration 3.3 Load dataset 3.4 Preprocessing 3.5 Clustering 3.6 Summarize results. 3.7 Conclusion 3.8 Save dataframe", " Chapter 3 Clustering 3.1 Libraries Library for R Markdown. library(rmarkdown) Library for clustering library(factoextra) Library for data frames processing. library(dplyr) Library for read dataset. library(readr) Library for data presentation. library(scales) Library for manage strings. library(stringr) 3.2 Configuration 3.2.1 Seed Set seed. set.seed(0) 3.2.2 Paths Set paths. path_import &lt;- &quot;data/preprocessed.csv&quot; path_export &lt;- &quot;data/clustering.csv&quot; 3.3 Load dataset virusTotal &lt;- read_csv(path_import) 3.4 Preprocessing 3.4.1 View dataset virusTotal %&gt;% paged_table() 3.4.2 Remove row number columns As can be seen, at read the col …1 is added, but the column n already has this information. Both of them are not interesting columns for clustering, so remove them. df &lt;- virusTotal %&gt;% select(-...1, -n) 3.4.3 Cast logical columns as numeric Cast logical to numeric in order to be able to cluster with them. df &lt;- df %&gt;% mutate_if(is.logical, as.numeric) 3.4.4 Select numeric columns Only numeric columns are suitable for clustering, so just remove them. df &lt;- df %&gt;% select_if(is.numeric) 3.4.5 Rows with NA rows_with_na &lt;- function(df){ df %&gt;% is.na() %&gt;% rowSums() %&gt;% sapply(function(x) x &gt; 0) %&gt;% which() } nrows_with_na &lt;- df %&gt;% rows_with_na() %&gt;% length() nrows_with_na / nrow(df) ## [1] 0.863388 Most of the rows of the dataframe has NA so can’t be deleted. If rows with NA were deleted most of the information will be loss. 3.4.6 Replace NA Clustering do not work with NAs because distance measure is needed. So just replace NA for predicted value using lineal model. remove_0 &lt;- function(x) x[x!=0] num_of_NA_by_column &lt;- function(df){ df %&gt;% is.na() %&gt;% colSums() } names_of_colums_with_NA &lt;- function(df) df %&gt;% num_of_NA_by_column() %&gt;% remove_0 %&gt;% names() lm_from_all_columns &lt;- function(df, column){ target_column_name &lt;- colnames(df)[column] col &lt;- df[,column] df &lt;- df %&gt;% select(., -all_of(names_of_colums_with_NA(.))) %&gt;% cbind(col) formula_lm &lt;- paste(target_column_name, &quot;.&quot;, sep = &quot; ~ &quot;) %&gt;% as.formula() lm(formula_lm, data = df, na.action = na.omit) } replace_NA_by_predicted_col &lt;- function(df, col_index){ col &lt;- df[, col_index] %&gt;% unlist() has_na &lt;- col %&gt;% anyNA() if(has_na){ lm_col &lt;- lm_from_all_columns(df, col_index) col_predicted &lt;- predict.lm(lm_col, df) %&gt;% as.double() na_col &lt;- col %&gt;% sapply(is.na) %&gt;% as.vector() col &lt;- if_else(na_col, col_predicted, col) } col &lt;- as.vector(col) return(col) } replace_NA_by_predicted &lt;- function(df){ for(index in 1:ncol(df)){ df[,index] &lt;- replace_NA_by_predicted_col(df, index) } return(df) } df &lt;- replace_NA_by_predicted(df) 3.4.7 View result df %&gt;% paged_table() 3.5 Clustering 3.5.1 K-Means 3.5.1.1 Optimal number of clusters Plot a graphic about the optimal number of clusters. df %&gt;% scale() %&gt;% fviz_nbclust(kmeans, method = &quot;wss&quot;, k.max = 15) As can be seen 3, 7 and 10 clusters seem to be optimal number of clusters. 3.5.1.2 Define a function cluster_k &lt;- function(df, k){ df_scale &lt;- df %&gt;% scale() %&gt;% as.data.frame() l &lt;- list() l$kmeans &lt;- kmeans(df_scale, centers = k, nstart = 25) l$plot &lt;- l$kmeans %&gt;% fviz_cluster(geom = &quot;point&quot;, data = df_scale) + ggtitle(paste(&quot;k&quot;, k, sep=&quot;=&quot;)) l$table &lt;- tibble(row = 1:nrow(df), cluster = l$kmeans$cluster, df) l$cluster_size &lt;- l$table %&gt;% group_by(cluster) %&gt;% summarise(size=n()) return(l) } 3.5.1.3 K-3 Apply kmeans with 3 clusters. cluster_k3 &lt;- cluster_k(df, 3) Show results cluster_k3$plot cluster_k3$table %&gt;% paged_table() cluster_k3$cluster_size %&gt;% paged_table() As can be seen in the table of the size of the clusters there are two main groups, but the other has only two virus in the cluster 2. 3.5.1.4 K-7 Apply kmeans with 7 clusters. cluster_k7 &lt;- cluster_k(df, 7) Show results cluster_k7$plot cluster_k7$table %&gt;% paged_table() cluster_k7$cluster_size %&gt;% paged_table() Again in k-7 clusters there are four main groups but the others has only 1 or 2 viruses. That is so curious. 3.5.1.5 K-10 Apply kmeans with 10 clusters. cluster_k10 &lt;- cluster_k(df, 10) Show results cluster_k10$plot cluster_k10$table %&gt;% paged_table() cluster_k10$cluster_size %&gt;% paged_table() Again in k-10 clusters there are many small groups that are very different from the rest in terms of number of viruses clasified in. 3.5.1.6 Conclusion This is so strange clustering distribution, so would be interesting to inspect why it is happening. 3.5.1.7 Add cluster information to the dataframe. df_clustering_info_added &lt;- df %&gt;% mutate( analysis_info.clustering.cluster.k3 = cluster_k3$kmeans$cluster) %&gt;% mutate( analysis_info.clustering.cluster.k7 = cluster_k7$kmeans$cluster) %&gt;% mutate( analysis_info.clustering.cluster.k10 = cluster_k10$kmeans$cluster) 3.5.2 Inspect small groups Define a function. small_clusters &lt;- function(table_cluster_size, percent){ max &lt;- max(table_cluster_size$size) min &lt;- min(table_cluster_size$size) threshold &lt;- min + (max - min) * percent small_clusters &lt;- table_cluster_size %&gt;% filter(size &lt;= threshold) %&gt;% select(cluster) %&gt;% unlist() return(small_clusters) } belongs_to_small_cluster &lt;- function(cluster, small_clusters) cluster %in% small_clusters 3.5.2.1 Small clusters of k10 Compute small clusters of k10 small_clusters_k10 &lt;- cluster_k10$cluster_size %&gt;% small_clusters(0.2) Show rows that belong to small clusters. belongs_to_small_cluster_k10 &lt;- belongs_to_small_cluster( cluster_k10$table$cluster, small_clusters_k10 ) small_groups_df &lt;- df %&gt;% filter(belongs_to_small_cluster_k10) small_groups_df %&gt;% paged_table() 3.5.2.2 Add this information to the dataframe. df_clustering_info_added &lt;- df_clustering_info_added %&gt;% mutate( analysis_info.clustering.belongs_to_small_cluster.k10 = belongs_to_small_cluster_k10) 3.5.3 Dendogram Calculate distances. distances &lt;- df %&gt;% scale() %&gt;% get_dist(method=&quot;euclidean&quot;) Generate dendogram. dendogram &lt;- distances %&gt;% hclust(method=&quot;ward.D&quot;) Plot dendogram. dendogram %&gt;% plot(main=&quot;Dendogram&quot;) ### Analyse clusters 3.5.3.1 Analyse variables by clusters The idea is view how affect belonging to a cluster to the main variables distribution. The main variables are those whose names do not begin with “something.”. So just keep columns whose name do not have ‘.’. cols_to_select &lt;- df_clustering_info_added %&gt;% colnames() %&gt;% str_detect(&quot;\\\\.&quot;, negate = TRUE) means_by_cluster &lt;- df_clustering_info_added %&gt;% group_by(analysis_info.clustering.cluster.k10) %&gt;% select_if(cols_to_select) %&gt;% summarise_all(mean) %&gt;% round(2) %&gt;% rename_with(function(name) paste(&quot;mean&quot;, name, sep = &quot;_&quot;)) %&gt;% rename(cluster = mean_analysis_info.clustering.cluster.k10) %&gt;% arrange(desc(mean_size)) means_by_cluster %&gt;% paged_table() 3.5.3.2 Analyse variables by cluster size The idea is view how affect the cluster size to the main variables distribution. means_by_cluster_size &lt;- df_clustering_info_added %&gt;% group_by(analysis_info.clustering.belongs_to_small_cluster.k10) %&gt;% select_if(cols_to_select) %&gt;% summarise_all(mean) %&gt;% round(2) %&gt;% rename_with(function(name) paste(&quot;mean&quot;, name, sep = &quot;_&quot;)) %&gt;% rename(small_cluster = mean_analysis_info.clustering.belongs_to_small_cluster.k10) %&gt;% mutate(small_cluster = as.logical(small_cluster)) %&gt;% arrange(desc(mean_size)) means_by_cluster_size %&gt;% paged_table() The result is amazing! The risk of the virus increase so much by belonging to a small group. Let’s express this in percentaje. increment_by_small_cluster &lt;- (means_by_cluster_size[1,] / means_by_cluster_size[2,]) %&gt;% select(-small_cluster) increment_by_small_cluster %&gt;% paged_table() increment_by_small_cluster_percentage &lt;- (increment_by_small_cluster - 1) %&gt;% select_if(function(x) !is.infinite(x)) %&gt;% unlist() %&gt;% percent() increment_by_small_cluster_percentage ## mean_size mean_community_reputation mean_malicious_votes ## &quot;715%&quot; &quot;43 700%&quot; &quot;6 600%&quot; ## mean_positives mean_times_submitted mean_total ## &quot;-12%&quot; &quot;-47%&quot; &quot;4%&quot; ## mean_total_PERMs mean_unique_sources ## &quot;283%&quot; &quot;1 514%&quot; 3.6 Summarize results. Looking at the results of the dendogram and knowing the distribution of clusters, seem that most of the viruses follows common patterns, but there are some others that breaks radically with these patterns. Virus that don’t follow patterns tend to: (size) Infect x8 more computers (community_reputation) Increase bad reputation x438 (harmless_votes) Are the only ones with harmless votes. (malicious_votes) Increase the chance to have malicious votes x67 (positives) Decrease positives by 12% (times_submitted) Decrease times submitted by 54% (total) Increase total by 3,75% (unique_source) Increase the unique sources x16 It is interesting to see how the most dangerous viruses: Are the only ones who have harmless votes. Request more permissions. 3.7 Conclusion Concluding, virus that don’t follow the commons patterns tends to be much more dangerous than the others. Signs of dangerous virus: Has high community reputation. Has harmless votes. Request many permissions. Has many unique sources. 3.8 Save dataframe Add the clustering information to the data frame. cluster_info_cols_index &lt;- df_clustering_info_added %&gt;% colnames() %&gt;% str_detect(&quot;^analysis_info&quot;) %&gt;% which() cluster_info_cols &lt;- df_clustering_info_added %&gt;% select(all_of(cluster_info_cols_index)) virusTotal_cluster &lt;- virusTotal %&gt;% select(-...1) %&gt;% cbind(cluster_info_cols) write.csv(virusTotal_cluster, path_export) "],["regression.html", "Chapter 4 Regression 4.1 Libraries 4.2 Load dataset 4.3 Preprocessing 4.4 Regression 4.5 View results 4.6 Conclusions", " Chapter 4 Regression 4.1 Libraries Library for R Markdown. library(rmarkdown) Library for data frames processing. library(dplyr) Library for read dataset. library(readr) Library for summary statistics. library(skimr) Library for plot. library(ggplot2) Library for regression. library(performance) Library for manage strings. library(stringr) Library for data presentation. library(scales) 4.2 Load dataset Set paths. path &lt;- &quot;data/clustering.csv&quot; virusTotal &lt;- read_csv(path) 4.3 Preprocessing 4.3.1 View dataset virusTotal %&gt;% paged_table() 4.3.2 Remove row number columns As can be seen, at read the col …1 is added, but the column n already has this information. df &lt;- virusTotal %&gt;% select(-...1, -n) 4.3.3 Cast logical columns as numeric Logical columns can be casted as numeric, TRUE as 1, FALSE as 0. But 0s are not suitable for regression, so let’s replace FALSE for -1 instead. replace_value &lt;- function(v, value, replacement){ v[v==value] &lt;- replacement return(v) } df &lt;- df %&gt;% mutate_if( is.logical, function(logical) replace_value(as.numeric(logical), 0, -1)) 4.3.4 Select numeric columns Only numeric columns are suitable for clustering, so just remove them. df &lt;- df %&gt;% select_if(is.numeric) 4.3.5 Rows with NA rows_with_na &lt;- function(df){ df %&gt;% is.na() %&gt;% rowSums() %&gt;% sapply(function(x) x &gt; 0) %&gt;% which() } nrows_with_na &lt;- df %&gt;% rows_with_na() %&gt;% length() nrows_with_na / nrow(df) ## [1] 0.863388 Most of the rows of the dataframe has NA so can’t be deleted. If rows with NA were deleted most of the information will be loss. 4.3.6 Replace NA Clustering do not work with NAs because distance measure is needed. So just replace NA for predicted value using lineal model. remove_0 &lt;- function(x) x[x!=0] num_of_NA_by_column &lt;- function(df){ df %&gt;% is.na() %&gt;% colSums() } names_of_colums_with_NA &lt;- function(df) df %&gt;% num_of_NA_by_column() %&gt;% remove_0 %&gt;% names() lm_from_all_columns &lt;- function(df, column){ target_column_name &lt;- colnames(df)[column] col &lt;- df[,column] df &lt;- df %&gt;% select(., -all_of(names_of_colums_with_NA(.))) %&gt;% cbind(col) formula_lm &lt;- paste(target_column_name, &quot;.&quot;, sep = &quot; ~ &quot;) %&gt;% as.formula() lm(formula_lm, data = df, na.action = na.omit) } replace_NA_by_predicted_col &lt;- function(df, col_index){ col &lt;- df[, col_index] %&gt;% unlist() has_na &lt;- col %&gt;% anyNA() if(has_na){ lm_col &lt;- lm_from_all_columns(df, col_index) col_predicted &lt;- predict.lm(lm_col, df) %&gt;% as.double() na_col &lt;- col %&gt;% sapply(is.na) %&gt;% as.vector() col &lt;- if_else(na_col, col_predicted, col) } col &lt;- as.vector(col) return(col) } replace_NA_by_predicted &lt;- function(df){ for(index in 1:ncol(df)){ df[,index] &lt;- replace_NA_by_predicted_col(df, index) } return(df) } df &lt;- replace_NA_by_predicted(df) 4.3.7 View result df %&gt;% paged_table() 4.4 Regression The idea is to get the best models to predict the most important variables of the dataset. How to get the best model? Compute all one variable models for target Sort them by R2-ajusted Iteratively while R2-adjusted does not get worse: Add the best predicted variable remaining from the sorted list to the lineal model. Check if there are useless variables to remove. Called as useless those predictors whose p-value is over 0.05. Remove useless if is necessary. Compute R2-adjusted. This method compute the best model? No, it’s no the best model, but it’s a good approach to get a good enough model without much computing. 4.4.1 Important columns The most important variables are those whose names do not begin with “something.”. So just keep columns whose name do not have ‘.’, excluding row number column “n”. important_columns_logical &lt;- df %&gt;% colnames() %&gt;% str_detect(&quot;n|\\\\.&quot;, negate = TRUE) important_columns_index &lt;- important_columns_logical %&gt;% which() important_columns_names &lt;- colnames(df)[important_columns_index] important_columns_names ## [1] &quot;size&quot; &quot;harmless_votes&quot; &quot;malicious_votes&quot; &quot;positives&quot; &quot;times_submitted&quot; ## [6] &quot;total&quot; &quot;total_PERMs&quot; 4.4.2 Define functions formula_from_names &lt;- function(target, col){ formulaStr &lt;- paste0(target, &quot;~&quot;, col) formula &lt;- as.formula(formulaStr) } lm_from_names &lt;- function(df, target, col) lm(formula_from_names(target, col), data = df) all_one_variable_models &lt;- function(df, target){ names &lt;- colnames(df) names &lt;- names[names!=target] lapply(names, function(col) lm_from_names(df, target, col)) } get_r2_adjusted &lt;- function(lm) r2(lm)[[2]] get_pvalues &lt;- function(lm, intercept = TRUE){ coefs_df &lt;- lm %&gt;% summary() %&gt;% coef() %&gt;% as.data.frame() pvales &lt;- coefs_df$`Pr(&gt;|t|)` names(pvales) &lt;- coefs_df %&gt;% rownames() %&gt;% str_replace(fixed(&quot;(Intercept)&quot;), &quot;1&quot;) if(!intercept){ pvales &lt;- pvales[2:length(pvales)] } return(pvales) } sort_models &lt;- function(models){ models_r2 &lt;- models %&gt;% sapply(get_r2_adjusted) models_df &lt;- tibble(model = models, r2 = models_r2) %&gt;% arrange(desc(r2)) %&gt;% pull(&quot;model&quot;) } rank_one_variable_models &lt;- function(df, target){ rank &lt;- df %&gt;% all_one_variable_models(target) %&gt;% sort_models return(rank) } get_target_name &lt;- function(lm){ lm_terms &lt;- lm_test %&gt;% summary() %&gt;% terms() predicted &lt;- lm_terms[[2]] %&gt;% as.character() return(predicted) } get_predictors_names &lt;- function(lm){ lm_terms &lt;- lm %&gt;% summary() %&gt;% terms() predictors_str &lt;- lm_terms[[3]] predictors &lt;- predictors_str %&gt;% as.character() %&gt;% str_remove_all(&quot;\\\\+|\\\\-|-[:blank:]1&quot;) %&gt;% str_split(fixed(&quot; &quot;)) %&gt;% unlist() predictors &lt;- predictors[predictors != &quot;&quot;] return(predictors) } lm_from_names &lt;- function(df, target, predictors, intercept = TRUE){ predictors_str &lt;- paste(predictors, collapse = &quot; + &quot;) formula_str &lt;- paste(target, predictors_str, sep = &quot; ~ &quot;) if(!intercept){ formula_str &lt;- paste(formula_str, &quot;1&quot;, sep = &quot; - &quot;) } formula &lt;- formula_str %&gt;% as.formula() lm &lt;- lm(formula, data = df) } best_model &lt;- function(df, target){ predictors_rank &lt;- rank_one_variable_models(df, target) %&gt;% sapply(get_predictors_names) idx &lt;- 1 r2_adjusted_prev &lt;- -Inf r2_adjusted_current &lt;- 0 lm_current &lt;- NA while(r2_adjusted_current &gt;= r2_adjusted_prev){ lm_current &lt;- lm_from_names(df, target, predictors_rank[1:idx]) pvalues &lt;- get_pvalues(lm_current, FALSE) useless &lt;- pvalues[pvalues &gt; 0.05] %&gt;% names() predictors_rank &lt;- predictors_rank[!(predictors_rank %in% useless)] idx &lt;- idx - length(useless) lm_current &lt;- lm_from_names(df, target, predictors_rank[1:idx]) r2_adjusted_prev &lt;- r2_adjusted_current r2_adjusted_current &lt;- get_r2_adjusted(lm_current) idx &lt;- idx + 1 } return(lm_current) } best_models &lt;- function(df, targets){ best_models &lt;- targets %&gt;% lapply(function(target) best_model(df, target)) return(best_models) } 4.4.3 Compute best lineal models Compute best models for important variables. best_models_main_variables &lt;- best_models(df, important_columns_names) 4.5 View results 4.5.1 R2-adjusted Calc R2-adjusted. best_models_r2 &lt;- best_models_main_variables %&gt;% sapply(get_r2_adjusted) names(best_models_r2) &lt;- important_columns_names best_models_r2 %&gt;% sort(decreasing = TRUE) ## harmless_votes malicious_votes total_PERMs size total positives ## 0.9990731 0.9942802 0.9816816 0.6683957 0.3840575 0.3141946 ## times_submitted ## 0.1166208 4.5.2 Summary best_models_summary &lt;- best_models_main_variables %&gt;% lapply(summary) names(best_models_summary) &lt;- important_columns_names best_models_summary$harmless_votes ## ## Call: ## lm(formula = formula, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.36657 -0.00136 0.00154 0.01933 0.02897 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5.931e-02 4.527e-03 -13.101 &lt; 2e-16 *** ## unique_sources 3.033e-02 9.403e-04 32.260 &lt; 2e-16 *** ## malicious_votes 1.951e-01 3.872e-02 5.038 1.15e-06 *** ## community_reputation 3.463e-03 9.708e-04 3.567 0.000464 *** ## additional_info.trid.MozillaArchiveFormat 3.051e-03 8.863e-04 3.442 0.000720 *** ## additional_info.compressedview.file_types.PNG 4.052e-05 1.485e-05 2.729 0.006991 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.04051 on 177 degrees of freedom ## Multiple R-squared: 0.9991, Adjusted R-squared: 0.9991 ## F-statistic: 3.923e+04 on 5 and 177 DF, p-value: &lt; 2.2e-16 best_models_summary$malicious_votes ## ## Call: ## lm(formula = formula, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.01685 -0.00534 -0.00534 -0.00534 0.97722 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.005335 0.005430 0.983 0.327 ## harmless_votes 0.688984 0.004705 146.429 &lt;2e-16 *** ## community_reputation -0.017440 0.001263 -13.811 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.07304 on 180 degrees of freedom ## Multiple R-squared: 0.9943, Adjusted R-squared: 0.9943 ## F-statistic: 1.582e+04 on 2 and 180 DF, p-value: &lt; 2.2e-16 best_models_summary$size ## ## Call: ## lm(formula = formula, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -41673771 -826734 -535594 831127 64617531 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 832680 816854 1.019 0.30942 ## additional_info.compressedview.file_types.unknown 69765 9050 7.709 8.98e-13 ## additional_info.compressedview.file_types.ELF 1983038 189875 10.444 &lt; 2e-16 ## additional_info.compressedview.file_types.Portable.Executable 701606 164994 4.252 3.43e-05 ## additional_info.trid.VymMindMap 2662922 375009 7.101 2.94e-11 ## additional_info.androguard.RiskIndicator.APK.SHARED.LIBRARIES -2335274 339745 -6.874 1.05e-10 ## additional_info.compressedview.file_types.HTML 2736559 949445 2.882 0.00444 ## ## (Intercept) ## additional_info.compressedview.file_types.unknown *** ## additional_info.compressedview.file_types.ELF *** ## additional_info.compressedview.file_types.Portable.Executable *** ## additional_info.trid.VymMindMap *** ## additional_info.androguard.RiskIndicator.APK.SHARED.LIBRARIES *** ## additional_info.compressedview.file_types.HTML ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9576000 on 176 degrees of freedom ## Multiple R-squared: 0.6793, Adjusted R-squared: 0.6684 ## F-statistic: 62.14 on 6 and 176 DF, p-value: &lt; 2.2e-16 best_models_summary$total ## ## Call: ## lm(formula = formula, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.7959 -0.7188 -0.0609 0.9391 2.9002 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 59.099805 0.185072 319.334 &lt; 2e-16 ## additional_info.trid.AndroidPackage 0.034405 0.004158 8.275 2.79e-14 ## additional_info.androguard.RiskIndicator.APK.SHARED.LIBRARIES 0.229673 0.035247 6.516 7.01e-10 ## ## (Intercept) *** ## additional_info.trid.AndroidPackage *** ## additional_info.androguard.RiskIndicator.APK.SHARED.LIBRARIES *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.575 on 180 degrees of freedom ## Multiple R-squared: 0.3908, Adjusted R-squared: 0.3841 ## F-statistic: 57.74 on 2 and 180 DF, p-value: &lt; 2.2e-16 best_models_summary$positives ## ## Call: ## lm(formula = formula, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.859 -2.185 -0.403 2.048 12.095 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20.95182 0.32890 63.703 &lt; 2e-16 ## additional_info.androguard.RiskIndicator.PERM.MONEY 2.95357 0.38510 7.670 1.06e-12 ## community_reputation -0.18760 0.05690 -3.297 0.00118 ## additional_info.androguard.RiskIndicator.APK.SHARED.LIBRARIES -0.54880 0.09193 -5.970 1.25e-08 ## ## (Intercept) *** ## additional_info.androguard.RiskIndicator.PERM.MONEY *** ## community_reputation ** ## additional_info.androguard.RiskIndicator.APK.SHARED.LIBRARIES *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.782 on 179 degrees of freedom ## Multiple R-squared: 0.3255, Adjusted R-squared: 0.3142 ## F-statistic: 28.79 on 3 and 179 DF, p-value: 3.057e-15 best_models_summary$times_submitted ## ## Call: ## lm(formula = formula, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -684.24 -8.07 -8.07 -7.07 3083.15 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.071 18.543 0.489 0.625 ## additional_info.androguard.RiskIndicator.PERM.EPHEMERAL 225.390 45.054 5.003 1.33e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 247.1 on 181 degrees of freedom ## Multiple R-squared: 0.1215, Adjusted R-squared: 0.1166 ## F-statistic: 25.03 on 1 and 181 DF, p-value: 1.331e-06 The most characteristic patterns are: Harmless_votes and malicious_votes are highly correlated between them, both has in common to be influenced by community_reputation. Size is correlated with the number of files some times, what make sense. Maybe a column of the total files would be a good way to predict the size. 4.5.3 Correlation between harmless_votes adn malicious_votes lm_votes &lt;- lm(malicious_votes ~ harmless_votes, data = df) summary(lm_votes) ## ## Call: ## lm(formula = malicious_votes ~ harmless_votes, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.01099 -0.01099 -0.01099 -0.01099 0.98901 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.010989 0.007749 1.418 0.158 ## harmless_votes 0.721612 0.005824 123.911 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1045 on 181 degrees of freedom ## Multiple R-squared: 0.9883, Adjusted R-squared: 0.9883 ## F-statistic: 1.535e+04 on 1 and 181 DF, p-value: &lt; 2.2e-16 df %&gt;% ggplot(aes(x = harmless_votes, y = malicious_votes)) + geom_point() + geom_smooth(method = &quot;lm&quot;) After plotting the results, that correlarion don’t seem as amazing as before. That correlation id due to both columns are almost full of 0s. percent_of_0s &lt;- function(x){ (sum(x == 0) / length(x)) %&gt;% percent() } percent_of_0s(df$harmless_votes) ## [1] &quot;99%&quot; percent_of_0s(df$malicious_votes) ## [1] &quot;98%&quot; 4.5.4 Correlarion between size and file types df$total_files &lt;- df %&gt;% select(contains(&quot;file_types&quot;)) %&gt;% rowSums() lm_size &lt;- lm(size ~ total_files, data = df) summary(lm_size) ## ## Call: ## lm(formula = size ~ total_files, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11929512 -5278138 -522546 1302322 160364387 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 528586 1510284 0.350 0.727 ## total_files 17463 3357 5.201 5.32e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15550000 on 181 degrees of freedom ## Multiple R-squared: 0.13, Adjusted R-squared: 0.1252 ## F-statistic: 27.05 on 1 and 181 DF, p-value: 5.322e-07 df %&gt;% ggplot(aes(y = size, x = total_files)) + geom_point() + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; The hypothesis is not met, the size don’t have a good correlarion with the number of files. 4.5.5 Correlarion between positives and money permission lm_positives &lt;- lm(positives ~ additional_info.androguard.RiskIndicator.PERM.MONEY, data = df) summary(lm_positives) ## ## Call: ## lm(formula = positives ~ additional_info.androguard.RiskIndicator.PERM.MONEY, ## data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.7574 -3.5732 -0.5732 2.4268 13.2426 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20.5732 0.3595 57.233 &lt; 2e-16 *** ## additional_info.androguard.RiskIndicator.PERM.MONEY 2.1842 0.3944 5.538 1.07e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.234 on 181 degrees of freedom ## Multiple R-squared: 0.1449, Adjusted R-squared: 0.1402 ## F-statistic: 30.67 on 1 and 181 DF, p-value: 1.065e-07 df %&gt;% ggplot(aes( y = positives, x = additional_info.androguard.RiskIndicator.PERM.MONEY)) + geom_point() + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; The hypothesis is not met, the positives don’t have a good correlarion with money permission. 4.6 Conclusions The only valuable conclusion is that the algorithm works well. The correlations make sense but most of the correlations happen because of there are many NAs and therefore few real data. "],["arules.html", "Chapter 5 Arules 5.1 Import of the VirusTotal dataset. 5.2 We select the rules. 5.3 We will discretize the values using quantiles. 5.4 The function does not apply to columns with few values, as they do not provide much information. 5.5 We eliminate the untouched columns. 5.6 We use the apriori method with standard confidence and support values. 5.7 We remove redundant rules. 5.8 Display", " Chapter 5 Arules library(dplyr) library(arules) library(magrittr) library(rmarkdown) library(readr) library(stringr) library(tidyverse) path_import = &quot;data/original.csv&quot; path_export = &quot;data/discretized.csv&quot; 5.1 Import of the VirusTotal dataset. In this exercise we are going to observe the access to permissions by files together with the risk they present. As there are a large number of viruses each with different objectives, the association rules generated can be used to predict which permissions a particular virus wants to access along with the probability that it will do so. We have also added the number of positives to see the risk factor of the different permit accesses. 5.1.1 We replace all the NA in the values of the columns permits, since we are going to relate them later with the numbers of positives. replace_na_which_colname_match &lt;- function(df, pattern, replacement){ cols_to_replace &lt;- df %&gt;% colnames() %&gt;% str_which(pattern) df_replaced_cols &lt;- df %&gt;% select(all_of(cols_to_replace)) %&gt;% sapply(function(col) replace_na(col, replacement)) df_without_replaced_cols &lt;- df %&gt;% select(-all_of(cols_to_replace)) return(cbind(df_without_replaced_cols, df_replaced_cols)) } pattern &lt;- &quot;additional_info.androguard.RiskIndicator.PERM&quot; virus &lt;- virus %&gt;% replace_na_which_colname_match(pattern, 0) 5.2 We select the rules. df_rules &lt;- virus %&gt;% select(c( positives, additional_info.androguard.RiskIndicator.PERM.DANGEROUS, additional_info.androguard.RiskIndicator.PERM.GPS, additional_info.androguard.RiskIndicator.PERM.NORMAL, additional_info.androguard.RiskIndicator.PERM.PRIVACY, additional_info.androguard.RiskIndicator.PERM.INTERNET, additional_info.androguard.RiskIndicator.PERM.INSTANT, additional_info.androguard.RiskIndicator.PERM.SMS, additional_info.androguard.RiskIndicator.PERM.MONEY, additional_info.androguard.RiskIndicator.PERM.SYSTEM, additional_info.androguard.RiskIndicator.PERM.CALL, additional_info.androguard.RiskIndicator.PERM.SIGNATURE, additional_info.androguard.RiskIndicator.PERM.DEVELOPMENT, additional_info.androguard.RiskIndicator.PERM.PRE23, additional_info.androguard.RiskIndicator.PERM.APPOP, additional_info.androguard.RiskIndicator.PERM.EPHEMERAL, additional_info.androguard.RiskIndicator.PERM.PREINSTALLED, additional_info.androguard.RiskIndicator.PERM.INSTALLER, additional_info.androguard.RiskIndicator.PERM.RUNTIME )) colnames(df_rules)&lt;-gsub(&quot;additional_info.androguard.RiskIndicator.PERM.&quot;,&quot;&quot;, colnames(df_rules)) df_rules %&gt;% paged_table() 5.3 We will discretize the values using quantiles. 5.4 The function does not apply to columns with few values, as they do not provide much information. labels &lt;- function(n){ if(n == 5){ return(c(&quot;very low&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;high&quot;, &quot;very high&quot;)) }else if(n == 4){ return(c(&quot;very low&quot;, &quot;low&quot;, &quot;high&quot;, &quot;very high&quot;)) }else if(n == 3){ return(c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;)) }else if(n == 2){ return(c(&quot;low&quot;, &quot;high&quot;)) }else{ stop(&quot;Not avalible&quot;) } } cut_by_quantiles &lt;- function(col){ quantiles &lt;- col %&gt;% na.omit() %&gt;% quantile() %&gt;% unique() if(length(quantiles) &gt; 2){ col &lt;- col %&gt;% cut(breaks = quantiles, labels = labels(length(quantiles)-1), include.lowest = TRUE) } return(col) } df_cut_by_quantiles &lt;- function(df){ df_without_numeric &lt;- df[sapply(df, function(col) !is.numeric(col))] df_numeric &lt;- df %&gt;% select_if(is.numeric) df_numeric &lt;- df_numeric %&gt;% lapply(cut_by_quantiles) return(cbind(df_without_numeric, df_numeric)) } df_test &lt;- df_rules %&gt;% df_cut_by_quantiles() 5.5 We eliminate the untouched columns. df_discretize &lt;- df_test %&gt;% select(c( 1,2,4,5,8,9)) df_discretize %&gt;% paged_table() write.csv(df_discretize, path_export) 5.6 We use the apriori method with standard confidence and support values. rules &lt;- apriori(df_discretize, parameter = list(supp=0.1, target=&quot;rules&quot;, conf=0.8)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen maxlen target ext ## 0.8 0.1 1 none FALSE TRUE 5 0.1 1 10 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 18 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[17 item(s), 183 transaction(s)] done [0.00s]. ## sorting and recoding items ... [17 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 4 5 6 done [0.00s]. ## writing ... [372 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. 5.7 We remove redundant rules. redundantes &lt;- is.redundant(rules) r_no_redundante &lt;- rules[!redundantes] inspect(head(r_no_redundante)) ## lhs rhs support confidence coverage lift count ## [1] {} =&gt; {MONEY=low} 0.8196721 0.8196721 1.0000000 1.000000 150 ## [2] {DANGEROUS=high} =&gt; {PRIVACY=high} 0.1038251 1.0000000 0.1038251 4.815789 19 ## [3] {PRIVACY=medium} =&gt; {DANGEROUS=low} 0.1092896 1.0000000 0.1092896 1.577586 20 ## [4] {MONEY=high} =&gt; {SMS=high} 0.1748634 0.9696970 0.1803279 3.857708 32 ## [5] {positives=high} =&gt; {SMS=low} 0.1584699 0.8055556 0.1967213 1.076034 29 ## [6] {PRIVACY=high} =&gt; {NORMAL=high} 0.1912568 0.9210526 0.2076503 4.435596 35 significantes &lt;- is.significant(r_no_redundante) r_significantes &lt;- r_no_redundante[significantes] inspect(head(r_significantes)) ## lhs rhs support confidence coverage lift count ## [1] {DANGEROUS=high} =&gt; {PRIVACY=high} 0.1038251 1.0000000 0.1038251 4.815789 19 ## [2] {PRIVACY=medium} =&gt; {DANGEROUS=low} 0.1092896 1.0000000 0.1092896 1.577586 20 ## [3] {MONEY=high} =&gt; {SMS=high} 0.1748634 0.9696970 0.1803279 3.857708 32 ## [4] {PRIVACY=high} =&gt; {NORMAL=high} 0.1912568 0.9210526 0.2076503 4.435596 35 ## [5] {NORMAL=high} =&gt; {PRIVACY=high} 0.1912568 0.9210526 0.2076503 4.435596 35 ## [6] {PRIVACY=high} =&gt; {SMS=high} 0.1857923 0.8947368 0.2076503 3.559497 34 5.8 Display library(arulesViz) subrules &lt;- subset(r_significantes, lift&gt;3) plot(subrules, method=&quot;graph&quot;, engine=&quot;htmlwidget&quot;, igraphLayout = &quot;layout_in_circle&quot;) We see that if the number of positives is low, the danger is medium, indicating that the antivirus used by VirusTotal is of good quality. Another curious relationship is between privacy and money, which show a high dependency. Because to access your money they must first know your bank account details. There is also a similar relationship between sms and privacy. "],["fca.html", "Chapter 6 FCA 6.1 Let’s try to find out which antivirus share the same graphic engine. To do this we must study how the other antiviruses behave when one detects the files as a virus. 6.2 we create a reduced dummie of the data frame to reduce the time it takes to find implications and concepts 6.3 We leave only the antivirus name 6.4 Transform boolean values to character to apply nominal scaling afterwards. 6.5 Null values symbolize that the file could not be processed. 6.6 We go through the columns and apply a nominal scaling. 6.7 Removes duplicated attributes and objects. 6.8 Implications and concepts with some examples 6.9 Avast acquired AVG in 2016, so they currently use the same engine with some different features. If our study has been carried out correctly, AVG=TRUE (detected as virus) should be a closure to Avast = TRUE. 6.10 Even with a reduced data frame, this assumption holds true. 6.11 Our study could also be used to look at the effectiveness of an antivirus. As long as we compare it with an antivirus that we consider reliable.", " Chapter 6 FCA library(fcaR) library(dplyr) library(readr) library(tidyverse) library(rmarkdown) path_import_o = &quot;data/original.csv&quot; virus_o &lt;- read_csv(path_import_o) ## New names: ## Rows: 183 Columns: 447 ## ── Column specification ## ───────────────────────────────────────────────────────────────────────────── Delimiter: &quot;,&quot; chr ## (158): vhash, sha256, type, scan_id, ssdeep, md5, permalink, sha1, verbose_msg, additional_i... dbl ## (197): ...1, document.id, total, size, times_submitted, harmless_votes, malicious_votes, uni... lgl ## (85): authentihash, additional_info.androguard.AndroidApplicationError, scans.Bkav.detected... dttm ## (7): scan_date, first_seen, last_seen, submission.date, additional_info.androguard.certifi... ## ℹ Use `spec()` to retrieve the full column specification for this data. ℹ Specify the column types ## or set `show_col_types = FALSE` to quiet this message. ## • `` -&gt; `...1` 6.1 Let’s try to find out which antivirus share the same graphic engine. To do this we must study how the other antiviruses behave when one detects the files as a virus. 6.2 we create a reduced dummie of the data frame to reduce the time it takes to find implications and concepts virus_o &lt;- virus_o %&gt;% select(contains(&#39;detected&#39;)) %&gt;% select(c(1:30,64)) %&gt;% slice(1:60) # we leave only the antivirus name colnames(virus_o)&lt;-gsub(&quot;scans.&quot;,&quot;&quot;,colnames(virus_o)) colnames(virus_o)&lt;-gsub(&quot;.detected&quot;,&quot;&quot;,colnames(virus_o)) 6.3 We leave only the antivirus name colnames(virus_o)&lt;-gsub(&quot;scans.&quot;,&quot;&quot;,colnames(virus_o)) colnames(virus_o)&lt;-gsub(&quot;.detected&quot;,&quot;&quot;,colnames(virus_o)) virus_o %&gt;% paged_table() 6.4 Transform boolean values to character to apply nominal scaling afterwards. 6.5 Null values symbolize that the file could not be processed. virus_detected &lt;- sapply(virus_o[, c(1:31)], as.character) # The na values correspond to data that could not be processed. virus_detected[is.na(virus_detected)]&lt;-&quot;UNABLE TO PROCESS&quot; virus_detected %&gt;% paged_table() 6.6 We go through the columns and apply a nominal scaling. #We create the formal context fc &lt;- FormalContext$new(virus_detected) for (i in 1:ncol(virus_o)){ fc$scale(colnames(virus_o[i]), type = &quot;nominal&quot;) } 6.7 Removes duplicated attributes and objects. 6.8 Implications and concepts with some examples fc$find_implications() fc$find_concepts() fc$implications[77] ## Implication set with 1 implications. ## Rule 1: {ALYac = FALSE, VIPRE = FALSE, Sangfor = FALSE, Arcabit = FALSE, Baidu = FALSE, Cyren = TRUE, ## BitDefender = FALSE, MicroWorld.eScan = FALSE, Ad.Aware = FALSE} -&gt; {K7GW = TRUE, Rising = FALSE} fc$concepts[16] ## A set of 1 concepts: ## 1: ({1, 2, 3, 4, 5, 6, 8, 10, 12, 13, 14, 15, 16, 19, 20, 21, 22, 23, 24, 25, 26, 28, 30, 33, 35, 39, 41, 42, 44, 46, 47, 48, 49, 51, 52, 54, 55, 56, 57, 58, 59, 60}, {CMC = FALSE, Malwarebytes = FALSE, K7AntiVirus = FALSE, ESET.NOD32 = TRUE, TrendMicro.HouseCall = FALSE, Cynet = TRUE, SUPERAntiSpyware = FALSE, Rising = FALSE, Ad.Aware = FALSE}) 6.9 Avast acquired AVG in 2016, so they currently use the same engine with some different features. If our study has been carried out correctly, AVG=TRUE (detected as virus) should be a closure to Avast = TRUE. S &lt;- Set$new(attributes = fc$attributes) S$assign(&quot;Avast = TRUE&quot; = 1) cat(&quot;Given the attribute:&quot;) ## Given the attribute: S ## {Avast = TRUE} cat(&quot;It has as closed::&quot;) ## It has as closed:: # Compute the intent of S fc$closure(S) ## {Bkav = FALSE, CMC = FALSE, ALYac = FALSE, Malwarebytes = FALSE, VIPRE = FALSE, K7AntiVirus = FALSE, ## Baidu = FALSE, ESET.NOD32 = TRUE, Avast = TRUE, SUPERAntiSpyware = FALSE, AVG = TRUE} 6.10 Even with a reduced data frame, this assumption holds true. 6.11 Our study could also be used to look at the effectiveness of an antivirus. As long as we compare it with an antivirus that we consider reliable. "],["social-network-analysis.html", "Chapter 7 Social Network Analysis 7.1 Libraries 7.2 Load dataset 7.3 Preprocessing 7.4 From data frame to graph", " Chapter 7 Social Network Analysis 7.1 Libraries Library for R Markdown. library(rmarkdown) Library for data frames processing. library(dplyr) Library for read dataset. library(readr) Library for graphs. library(igraph) 7.2 Load dataset Set paths. path &lt;- &quot;data/clustering.csv&quot; virusTotal &lt;- read_csv(path) 7.3 Preprocessing 7.3.1 View dataset virusTotal %&gt;% paged_table() 7.3.2 Remove extra row number columns As can be seen, at read the col …1 is added, but the column n already has this information. df &lt;- virusTotal %&gt;% select(-...1) 7.4 From data frame to graph The objective is to generate a graph from existing data frame. But how? The idea is create relationships between virus which has same values. Also give weights to relationships as a function of how likely it is that coincidence will occur for that variable. Plan: Find virus with sames values in any variable. Compute how likely it is that coincidence. Set the Weight of that relationship based on that probability. What is needed? Create a table with the two first columns with the indexes of the viruses in the relationship. The rest of columns are the values that has in common. Compute a list with the frequency of each element for every column. Add the column Weight to the data frame based in a frecuency ponderation of each value in common. Create a graph from it. 7.4.1 Find matching values pairs &lt;- function(n){ pairs &lt;- list() for(i in 1:(n-1)){ for(j in (i+1):n){ pairs[[length(pairs) + 1]] &lt;- c(i, j) } } names(pairs) &lt;- pairs %&gt;% sapply(function(v) paste(as.character(v), collapse = &quot;-&quot;)) return(pairs) } df_ij &lt;- function(df, i, j){ df_ij &lt;- (df[i,] == df[j,]) %&gt;% ifelse(as.vector(df[i,]), NA) %&gt;% unlist() %&gt;% c(i, j, .) names(df_ij) &lt;- c(&quot;v1&quot;, &quot;v2&quot;, colnames(df)) return(df_ij) } df_edges &lt;- df[1:10] %&gt;% ncol() %&gt;% pairs() %&gt;% sapply(function(v) df_ij(df[1:2,], v[1], v[2])) %&gt;% rbind() df_edges %&gt;% paged_table() Not endded. "]]
