---
title: "Regression"
author: "Pedro Antonio Benito Rojano"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Regression

## Libraries

Library for R Markdown.

```{r}
library(rmarkdown)
```

Library for data frames processing.

```{r message=FALSE}
library(dplyr)
```

Library for read dataset.

```{r}
library(readr)
```

Library for summary statistics.

```{r}
library(skimr)
```

Library for plot.

```{r}
library(ggplot2)
```

Library for regression.

```{r}
library(performance)
```

Library for manage strings.

```{r}
library(stringr)
```

Library for data presentation.

```{r message=FALSE}
library(scales)
```

## Load dataset

Set paths.

```{r}
path <- "data/clustering.csv"
```

```{r message=FALSE}
virusTotal <- read_csv(path)
```

## Preprocessing

### View dataset

```{r}
virusTotal %>% paged_table()
```

### Remove row number columns

As can be seen, at read the col ...1 is added, but the column n already has this information.

```{r}
df <- 
  virusTotal %>% 
  select(-...1, -n)
```

### Cast logical columns as numeric

Logical columns can be casted as numeric, TRUE as 1, FALSE as 0.

But 0s are not suitable for regression, so let's replace FALSE for -1 instead.

```{r}
replace_value <- 
  function(v, value, replacement){
    v[v==value] <- replacement
    return(v)
  }
```

```{r}
df <- 
  df %>% 
  mutate_if(
    is.logical, 
    function(logical) replace_value(as.numeric(logical), 0, -1))
```

### Select numeric columns

Only numeric columns are suitable for clustering, so just remove them.

```{r}
df <- 
  df %>% 
  select_if(is.numeric)
```

### Rows with NA

```{r}
rows_with_na <- 
  function(df){
      df %>% 
      is.na() %>% 
      rowSums() %>% 
      sapply(function(x) x > 0) %>% 
      which()
  }
```

```{r}
nrows_with_na <- 
  df %>% 
  rows_with_na() %>% 
  length()

nrows_with_na / nrow(df)
```

Most of the rows of the dataframe has NA so can't be deleted. If rows with NA were deleted most of the information will be loss.

### Replace NA

Clustering do not work with NAs because distance measure is needed. So just replace NA for predicted value using lineal model.

```{r}
remove_0 <- 
  function(x) x[x!=0]
```

```{r}
num_of_NA_by_column <- 
  function(df){
    df %>% is.na() %>% colSums()
  }
```

```{r}
names_of_colums_with_NA <- 
  function(df)
    df %>% 
      num_of_NA_by_column() %>% 
      remove_0 %>% 
      names()
```

```{r}
lm_from_all_columns <- 
  function(df, column){
    target_column_name <- 
      colnames(df)[column]
    col <- df[,column]
    df <- 
      df %>% 
      select(., -all_of(names_of_colums_with_NA(.))) %>% 
      cbind(col)
    formula_lm <- 
      paste(target_column_name, ".", sep = " ~ ") %>% 
      as.formula()
    lm(formula_lm, data = df, na.action = na.omit)
  }
```

```{r}
replace_NA_by_predicted_col <- 
  function(df, col_index){
    col <- 
      df[, col_index] %>% 
      unlist()
    has_na <- 
      col %>% 
      anyNA()
    if(has_na){
      lm_col <- 
        lm_from_all_columns(df, col_index)
      col_predicted <- 
        predict.lm(lm_col, df) %>% 
        as.double()
      na_col <- 
        col %>% 
        sapply(is.na) %>% 
        as.vector()
      col <- 
        if_else(na_col, col_predicted, col)
    }
    col <- 
      as.vector(col)
    return(col)
  }
```

```{r}
replace_NA_by_predicted <- 
  function(df){
    for(index in 1:ncol(df)){
      df[,index] <- replace_NA_by_predicted_col(df, index)
    }
    return(df)
  }
```

```{r warning=FALSE}
df <- 
  replace_NA_by_predicted(df)
```

### View result

```{r}
df %>% 
  paged_table()
```

## Regression

The idea is to get the best models to predict the most important variables of the dataset.

How to get the best model?

1. Compute all one variable models for target
2. Sort them by R2-ajusted
3. Iteratively while R2-adjusted does not get worse:
- Add the best predicted variable remaining from the sorted list to the lineal model.
- Check if there are useless variables to remove. Called as useless those predictors whose p-value is over 0.05.
- Remove useless if is necessary.
- Compute R2-adjusted.

This method compute the best model?

No, it's no the best model, but it's a good approach to get a good enough model without much computing.

### Important columns

The most important variables are those whose names do not begin with "something.". So just keep columns whose name do not have '.', excluding row number column "n".

```{r}
important_columns_logical <- 
  df %>% 
  colnames() %>% 
  str_detect("n|\\.", negate = TRUE)
```

```{r}
important_columns_index <- 
  important_columns_logical %>% 
  which()
```

```{r}
important_columns_names <- 
  colnames(df)[important_columns_index]
important_columns_names
```
### Define functions

```{r}
formula_from_names <- 
  function(target, col){
    formulaStr <- paste0(target, "~", col)
    formula <- as.formula(formulaStr)
  }
```

```{r}
lm_from_names <- 
  function(df, target, col)
    lm(formula_from_names(target, col), data = df)
```

```{r}
all_one_variable_models <- function(df, target){
  names <- colnames(df)
  names <- names[names!=target]
  lapply(names, function(col) lm_from_names(df, target, col))
}
```

```{r}
get_r2_adjusted <- 
  function(lm)
    r2(lm)[[2]]
```

```{r}
get_pvalues <-
  function(lm, intercept = TRUE){
    coefs_df <- 
      lm %>% 
      summary() %>% 
      coef() %>% 
      as.data.frame()
    pvales <- 
      coefs_df$`Pr(>|t|)`
    names(pvales) <- 
      coefs_df %>% 
      rownames() %>% 
      str_replace(fixed("(Intercept)"), "1")
    if(!intercept){
      pvales <- 
        pvales[2:length(pvales)]
    }
    return(pvales)
  }
```

```{r}
sort_models <- function(models){
  models_r2 <- 
    models %>% 
    sapply(get_r2_adjusted)
  models_df <- 
    tibble(model = models, r2 = models_r2) %>% 
    arrange(desc(r2)) %>% 
    pull("model")
}
```

```{r}
rank_one_variable_models <- 
  function(df, target){
    rank <- 
      df %>% 
      all_one_variable_models(target) %>% 
      sort_models
    return(rank)
  }
```

```{r}
get_target_name <- 
  function(lm){
    lm_terms <- 
      lm_test %>% 
      summary() %>% 
      terms()
    predicted <- 
      lm_terms[[2]] %>% 
      as.character()
    return(predicted)
  }
```

```{r}
get_predictors_names <- 
  function(lm){
    lm_terms <- 
      lm %>% 
      summary() %>% 
      terms()
    predictors_str <- 
      lm_terms[[3]]
    predictors <- 
      predictors_str %>% 
      as.character() %>% 
      str_remove_all("\\+|\\-|-[:blank:]1") %>% 
      str_split(fixed(" ")) %>% 
      unlist()
    predictors <- 
      predictors[predictors != ""]
    return(predictors)
  }
```

```{r}
lm_from_names <- 
  function(df, target, predictors, intercept = TRUE){
    predictors_str <- 
      paste(predictors, collapse = " + ")
    formula_str <- 
      paste(target, predictors_str, sep = " ~ ")
    if(!intercept){
      formula_str <- 
        paste(formula_str, "1", sep = " - ")
    }
    formula <- 
      formula_str %>% 
      as.formula()
    lm <- 
      lm(formula, data = df)
  }
```

```{r}
best_model <- 
  function(df, target){
    predictors_rank <- 
      rank_one_variable_models(df, target) %>% 
      sapply(get_predictors_names)
    idx <- 1
    r2_adjusted_prev <- -Inf
    r2_adjusted_current <- 0
    lm_current <- NA
    while(r2_adjusted_current >= r2_adjusted_prev){
      lm_current <- 
        lm_from_names(df, target, predictors_rank[1:idx])
      pvalues <- 
        get_pvalues(lm_current, FALSE)
      useless <- 
        pvalues[pvalues > 0.05] %>% 
        names()
      predictors_rank <- 
        predictors_rank[!(predictors_rank %in% useless)]
      idx <- idx - length(useless)
      lm_current <- 
        lm_from_names(df, target, predictors_rank[1:idx])
      r2_adjusted_prev <- 
        r2_adjusted_current
      r2_adjusted_current <- 
        get_r2_adjusted(lm_current)
      idx <- idx + 1
    }
    return(lm_current)
  }
```

```{r}
best_models <- 
  function(df, targets){
    best_models <- 
      targets %>% 
      lapply(function(target) best_model(df, target))
    return(best_models)
  }
```

### Compute best lineal models

Compute best models for important variables.

```{r}
best_models_main_variables <- 
  best_models(df, important_columns_names)
```

## View results

### R2-adjusted

Calc R2-adjusted.

```{r}
best_models_r2 <- 
  best_models_main_variables %>% 
  sapply(get_r2_adjusted)
names(best_models_r2) <- 
  important_columns_names
```

```{r}
best_models_r2 %>% 
  sort(decreasing = TRUE)
```
### Summary

```{r}
best_models_summary <- 
  best_models_main_variables %>% 
  lapply(summary)
names(best_models_summary) <- 
  important_columns_names
```

```{r}
best_models_summary$harmless_votes
best_models_summary$malicious_votes
best_models_summary$size
best_models_summary$total
best_models_summary$positives
best_models_summary$times_submitted
```

The most characteristic patterns are:

- Harmless_votes and malicious_votes are highly correlated between them, both has in common to be influenced by community_reputation.
- Size is correlated with the number of files some times, what make sense. Maybe a column of the total files would be a good way to predict the size.

### Correlation between harmless_votes adn malicious_votes

```{r}
lm_votes <-
  lm(malicious_votes ~ harmless_votes, data = df)
```

```{r}
summary(lm_votes)
```
```{r eval=FALSE}
df %>% 
  ggplot(aes(x = harmless_votes, y = malicious_votes)) +
  geom_point() + 
  geom_smooth(method = "lm")
```

After plotting the results, that correlarion don't seem as amazing as before. That correlation id due to both columns are almost full of 0s.

```{r}
percent_of_0s <- 
  function(x){
    (sum(x == 0) / length(x)) %>% 
    percent()
  }
```

```{r}
percent_of_0s(df$harmless_votes)
percent_of_0s(df$malicious_votes)
```

### Correlarion between size and file types

```{r}
df$total_files <- 
  df %>% 
  select(contains("file_types")) %>% 
  rowSums()
```

```{r}
lm_size <- 
  lm(size ~ total_files, data = df)
```

```{r}
summary(lm_size)
```

```{r}
df %>% 
  ggplot(aes(y = size, x = total_files)) +
  geom_point() + 
  geom_smooth(method = "lm")
```

The hypothesis is not met, the size don't have a good correlarion with the number of files.

### Correlarion between positives and money permission

```{r}
lm_positives <- 
  lm(positives ~ additional_info.androguard.RiskIndicator.PERM.MONEY, data = df)
```

```{r}
summary(lm_positives)
```

```{r}
df %>% 
  ggplot(aes(
    y = positives, 
    x = additional_info.androguard.RiskIndicator.PERM.MONEY)) +
  geom_point() + 
  geom_smooth(method = "lm")
```

The hypothesis is not met, the positives don't have a good correlarion with money permission.

## Conclusions

The only valuable conclusion is that the algorithm works well.

The correlations make sense but most of the correlations happen because of there are many NAs and therefore few real data. 











